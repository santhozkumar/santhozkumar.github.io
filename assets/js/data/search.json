[ { "title": "Poetry Commands", "url": "/posts/Poetry/", "categories": "Development", "tags": "python, notes", "date": "2023-10-10 21:31:00 +0530", "snippet": "## Start a Projectcreate a project in new folder with the package name and the boilerplatepoetry new my-packageorcreate the pyproject.toml in the current folder interactivelypoetry initadd a package to the projectpoetry add requests pendulumUpade the packageIn order to get the latest versions of the dependencies and to update the poetry.lock filepoetry update requests tomlshow the packages installedpoetry showshow the particular package dependenciespoetry show &lt;package-name&gt;To export the requirements.txtformat can be ‘constrainst.txt’ or ‘requriements.txt’poetry export -f requirements.txt --output requirements.txt" }, { "title": "Docker and Kubernetes", "url": "/posts/TypeScript/", "categories": "Development, Frontend", "tags": "ts, notes", "date": "2023-10-10 21:31:00 +0530", "snippet": "The BasicsTo install the typescript compiler globally npm install -g typescriptEmitting with Errors, even tsc reports errors it still emits js files to stop it run tsc --noEmitOnError hello.tsErased Types, after processing ts files with tsc, type annotations are erased, it can also downlevel the js code to ES3 version by default. we can set the target using this command tsc --target es2015 hello.tsStrictnessnoImplicitAny Turning on the noImplicitAny flag will issue an error on any variables whose type is implicitly inferred as any.strictNullChecks By default, values like null and undefined are assignable to any other type" }, { "title": "Systemd learning", "url": "/posts/systemd/", "categories": "Development", "tags": "linux, service", "date": "2023-08-05 21:31:00 +0530", "snippet": "What is ‘Systemd’? It’s an init system The init system is the process running on your server(PID 1) It manages all services that run in the backgroundWorking with Units Units in Systemd are resources that it’s able to manage. These includes services, timers, mounts, automounts, and there’s moreFrequent commandsStart/Stop/Servicesystemctl status &lt;service-name&gt; to check the statussystemctl start &lt;service-name&gt; to start the servicesystemctl stop &lt;service-name&gt; to stop the servicesystemctl restart &lt;service-name&gt; to restart the serviceEnable/Disablesystemctl enable &lt;service-name&gt; to start the service while bootingsystemctl disable &lt;service-name&gt; to not start the service while bootingenabled by default in ubuntu/Debian, disabled by defaults in Fedora/CentosWhere are unit files stored?service files are text files that contain instructions on how systemd needs to manage the service.Systemd Unit Directories /etc/systemd/system – /run/systemd/system – contains runtime systemd units /lib/systemd/system – anytime you install a library and it contains systemd service file, that’s stored hereIf all three directories conains a same service file name, it takes priority as above order.when need to override the config of the installed lib service, write a new service file in the higher priority unit directories. Because when updating the library via apt, the service file in lib will be overwritten." }, { "title": "Docker and Kubernetes", "url": "/posts/Docker/", "categories": "Development, Testing", "tags": "docker, kubernetes, test", "date": "2022-06-28 21:31:00 +0530", "snippet": "DockerDocker --version to know the version of docker installed. Layers of images Mostly Linux Base image, because small in size e.g. Alpine Application image on topDocker Ecosystem Docker server — where docker is installed Docker Client — Cli Docker Daemon — Docker Service Docker machine — is a tool to create small VM’s on Dcoker Docker compose - too to create service, stack of services Swarm — Orchestration tool by Docker. Alternatives Kubernetes or Apache Mesos Registry — where docker images are availableCloud:compute — underline hardware we can run applicationsAWS: EC2, lambda, Elastic beanstalk, EKS,GCP: compute-engine, cloud-function App-engine, GKSContainer :A way to package application with all the necessary dependecies and configurationContainers live in? container Repository private repositories public repository for Docker — DockerHubBefore containers configuration on the sever needed dependency version conflicts textual guide of deployment misunderstandingsAftr Containers Developers and opeartions work together to package the applccation in a container no environmental configuration needed on the serverDifference between Docker and VMDocker virtualizes the applicaiton layer and uses the kernel of the host, while VM virtualizes the OS kernel and ApplicationsDocker toolbox allows you to run the docker images of windows on linux and viceversaDocker Image :Image is a template for creating a environment of our choice. often a snapshot. it contains everything an app needs to run.remove all the dangling images () `docker rmi $(docker images -f \"dangling=true\" -q)`Container :Running an instance of an imageDocker Hub :It is simply a registry, where you can download images and push your own images for others to use it.docker pull image-name : tag-name to pull an image from the dockerhubdocker image rm image-id to delete the pulled images or docker rmi image-iddocker image ls to check the images downloaded locally.docker run image-name : tag-name to run the container of an image inside docker. it will hang so need to run in detach mode. docker run -d image-name : tag-namedocker container ls or docker ps to check the containers running in the docker.docker stop container-id or to stop the container running.Exposing Port:Mapping the localhost port 8080 to the Container port 80docker run -d -p 8080:80 nginx:latestdocker run -d -p 8080:80 -p 3000:80 nginx:latest Mapping multiple ports of local host to the docker container.Managing Containers:Whenever we run the Docker run command a new container id and name is created. So it is advisable not to use the docker run command once the first container is created.use the docker start and docker stop commands to start and stop the existing conatainersdocker ps -a to list all the containers which have been created.docker rm container-id or docker rm container-name to delete the created containersdocker rm $(docker ps -aq) to remove all the containers which have been created. but won’t remove the running container. use force rm to remove the running also docker rm -f $(docker ps -aq)docker run —name website -d -p 8080:80 -p 3000:80 nginx:latest to name the container user the --name optionsDocker PS –Format: docker ps —format=\"ID\\\\t{{.ID}}\\nName\\\\t{{.Names}}\\nImage\\\\t{{.Image}}\\nPorts\\\\t{{.Ports}}\\nCommand\\\\t{{.Command}}\\nCreated\\\\t{{.CreatedAt}}\\nStatus\\\\t{{.Status}}\\n” or set FORMAT=’string’. then access the FORMAT env variable in the commandVolumesFor every docker, new volume will be created if not specified. and Stored under /var/lib/docker/volume. To get more storage, mount this directory to the Storage capacity and use the created volumesdocker volume create volume-nameallows sharing of data. Files and Folders.Between host and container. Between containersdocker run --name website -v source:destination:ro -d -p 8080:80 image-namedocker run --name website -v $(pwd):/usr/share/nginx/html:ro -d -p 8080:80 nginxThree types : Mount with the path from the host file system Anonymous volumes Named volumesMounted volumes are stored in /var/lib/docker/volumesVolumes between host and containerdocker exec -it website bash to open the container in the interactive mode and start with bash terminalVolumes(Between Containers)docker run --volumes-from website --name website-copy -d -p 8081:80 nginxDockerFile Build own images, it contains list of steps of how to create our image Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession. FROM nginx:latestADD . /usr/share/nginx/htmlIAC - Infrastructure as Code Ansible Teraform Powershell Chef Puppet Jenkins Pile ScriptingDocker BuildDockerfile is used to create custom imagedocker build --tag website:latest . —&gt; this command looks for the dockerfile in the current directory and build the imageReference documentationFROM -ADD - to add file in the working directoryRUN -CMD - to copy file from base machine to containerWORKDIR - set the working directory for the container, if not exists, it will create new onePython implementation of Docker fileFROM python:latestWORKDIR /appCOPY . /appRUN pip install —trusted-host pypi.python.org -4 requirements.txtEXPOSE 80ENV NAME WorldCMD [“python”, “app.py”].dockerignorenode_modulesDockerfile.git*.gulp.jsfolder/*.exeCaching and LayersEvery step in dockerfile is a layer, layer can be used to cachingAlpine — linux editioncan be used to reduce the sizze of the imagePulling alpine imagesDocker pull image-name:tag-nameTags, Versionsingdocker build -t santhosh-website:latest .docker tag santhosh-website:latest santhosh-website:1After tag command image id will be same.Docker RegistryHighly scalable server side application that stores and lets you distribute Docker imagesUsed in CD/CI pipelineRun your applicationsPrivate/Public Registries available. Docker Hub quay.io Amazon ECRpushing images to docker hubYou can push a new image to this repository using the CLIdocker tag local-image:tagname new-repo:tagnamedocker push new-repo:tagnamePulling own imagesdocker pull santozkumar/website:latestInspect commanddocker inspect image-id or docker inspect container-idDocker Logsdocker logs container-iddocker logs -f container-idDocker EXecdocker exec -it container-id /bin/bashDocker Network ifconfig -a more below docker0 supplies the ip-address to all the containers in the netwokdocker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255inet6 fe80::42:48ff:feb2:c741 prefixlen 64 scopeid 0x20ether 02:42:48:b2:c7:41 txqueuelen 0 (Ethernet)RX packets 17 bytes 5206 (5.2 KB)RX errors 0 dropped 0 overruns 0 frame 0TX packets 51 bytes 7535 (7.5 KB)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0docker network ls to check the list of docker netword createddocker network create new-network-name to create a new docker networddocker network inspect new-network-name to inspect the subnet of the created networkdocker network disconnect network-name container-name to disconnectdocker network connect network-name container-name to connectdocker network rm network-name to delete the created networkdocker network prune remove all unused networksdocker run -d --network some-network --name some-mongo \\ -e MONGO_INITDB_ROOT_USERNAME=mongoadmin \\ -e MONGO_INITDB_ROOT_PASSWORD=secret \\ mongoDocker ComposeCompose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.Docker-compose upDocker SwarmHardware requirements manager/master node worker nodes(atleast 2)Software requirments in all machine docker must run ( systemctl status docker) all machines are interconnected (ip a) to check ip address of all machines cat /etc/hosts add other hosts ip in this file with names ping -c 3 docker-master all the machines are ssh enableddocker node ls to check the nodes connected in the docker-swarm cluster (i.e. status of cluster)docker swarm leave to leave from the clusterdocker swarm init --advertise-addr ip-address-value to initialize the cluster, this node will be assigned as manager. and it shows the command to add the worker and other manager node to this clusterDeplying a service into the swarm cluster.docker service create —replicas 7 -p 80:80 nginxabove command creates 7 containers of nginx in the cluster.docker stack deploy -c docker-compose.yaml myapp-stachdocker node inspect self to check the ip-address and name and configs of the hostdocker node inspect docker-worker1 configs of other host in clusterdocker node update —availability drain docker-worker1Kubernetes Provisioning and deployment of containers Redundancy and availability of containers Scaling up or removing containers to spread application load evenly actoss Movement of containers from one host to another if there is a shortage Allocation of resources between containers External exposure of services running in a container with the outside world Load balancing of service discovery b/w containers Health monitoring and logsIn Docker Swarm containers split including master nodes, but in kubernetes master node doesn’t run any app containers, it run maximun no of internal processesKubernetes is the most popular container orchestration toolOfficial definition :open source container orchestration tool, developed by google.helps you manage containerized applications in different deployment environments (physical, virtual and hybrid)what problem does Kubernetes solve?increasing usage of Microservices and able to solve the increased usage of containerswhat features do orchestration tool offer?High availability or no downtimeScalability or high performanceDisaster recovery - backup and restoreKubernetes ArchitectureCluster group of application instances - SWARM, Kubernetes, Apache mesos group of database instances - RAC group of servers - SUN, Veritas, Redhat Cluster, IBM cluster Advantages of cluster : High availaility auto-scaling auto-healing load-balancingMaster node will be connected to the couple of worker nodes, where each worker node has the Kubelet process running it.Kubelet is a kubernetes process that makes it possible for the cluster to talk to each other and execute some tasks on nodes.On worker Nodes our applications are running. Much bigger more resources.On Master Nodes, important K8s processes are running. more important. so have the backup of master in prod environments.Kubernetes master: the master manages the scheduling and deployment of application instances across nodes, and the full set of services the master node runs is know as the control plane. The master communicates with nodes through the kubernetes API server. The scheduler assigns nodes to pods depending on the resource and policy constraints you’ve defined.Kubelet : Each node runs an agent process called a kubelet, that’s responsible for managing the state of the node: starting, stopping and maintaining application containers based on instructions from control plane. It receives information from the kubernetes API server. API Server - which also a contianer, entrypoint to K8S cluster, different kubernetes clients will talk to this via UI, API, or CLI. Controller manager - keeps track of whats happening in the cluster. detects cluster changes i.e. if any pod dies. Controller Manager —&gt; Scheduler —&gt; where(node) to put pod? —&gt; Kubelet Scheduler - scheduling containers on different nodes based on work load, available server resources. ensures Pods placement. Schedule new pod —&gt; API Server —&gt; Scheduler —&gt; where to put the pod? —&gt; kubelet starts the pod etcd — basically holds anytime the current state of the cluster. holds all the configuration data, state data of each container on each node. Backup and restore is made from this etcd snapshots kubernetes backing key-value store. What resources are available?Did the cluster state change?is the cluster healthy?all these informations are stored in etcd.Virtual Network which enables master and work nodes to talk to each other. Creates on unified machine. each node has multiple Pods on it 3 processes must be installed on every Node, that are used to schedule and manage those pods Container runtime — like Docker or something Kubelet interacts with both - the container and node. it schedules the process. it starts the pod with a container inside Kube Proxy - forwarding requests from services to Pods. Chapter 2. The Transport Layer: TCP, UDP, and SCTP - Shichao’s Notes Services — communication between the Pods is handled How do you interact with this cluster?Kubectl setup :echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrcsource .bashrchttps://kubernetes.io/docs/concepts/services-networking/connect-applications-service/10:18kubernetes.io/docs/tutorials/stateless-application/guestbook10:19https://labs.play-with-k8s.com/10JobA Job creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.Pod, Services and IngressNode — physical or virtual serverPod — smallest unit of K8s Abstraction over container usually 1 application per Pod Each Pod gets it’s own IP address New IP address on re-creation — it;s a disadvantage when Pod is dead, new ip created. So service helps to overcome thisService : Permanent IP address attached to each Pod Also acts as a load balancer lifecycle of Pod and Service not connected. even pod dies, ip address stay.Ingress :request first goes to ingress, it does the forward invent to the service.allows to name the website. image Config Map and SecretConfigMap : external configuration of your application DB_URL = mongo-dbSecret : used to store secret data, stored in base64 encoded. DB_USER = mongo-user or DB_PASSWORD = mongo-passwordUse the above as environment variables or as a properties file.Volumes:if the DB restarted, data is gone. so attach a local or remote storage volume to the DB Pod.K8S doesn’t manage data persistance, our responsible to backup or replicate the data in external hardwareDeployment and Stateful SetReplicate Everything on multiple nodesDeployment :Replica is connected to the same Service(permanent Ip, load balancer)define blueprints for my-app pods —&gt; which is called as deploymentsdeployment is the abstraction of pods.DB can’t be replicated via Deployment. because DB have state, so use Statefulset to avoid data inconsistencies.StatefulSET :for StateFull apps or Databases. which read and writes are synchronized.Deploying Statefulset not easy, so often host DB outside of K8s cluster and do deployents of Stateless applications.Benefits of K8s. High availability and scalability Every component is replicated, load balanced, no bottleneck that slows down the responses Components summary:POD - abstraction of containersService - CommunicationIngress - route traffic into clusterConfigMap and Secrets - External ConfigurationVolumes - Data persistanceDeployment and StatefulSet — Replicating mechanismsMinikube and Kubectlkubeadm is utility to create multi-node clusterminikube is utility to create single-node clusterkubectl is a cli/utility to communicate with k8s cluster Structure What is Minikube?Test/Local cluster setupcreates virtual box on your laptopNode runs in that Virtual Box1 Node k8s clusterfor testing purposeswhat is Kubectl?command line tool for any type for k8s clusterMinikube master : kube-apiserver Etcd Node-Controller Replica-controllerWorker node Kubelet Container runtime kube-proxyKubeminikube start —vm-driver=hyperkit to start the kubernetes clusterkubectl get nodes or kubectl get nodes -o wideget the status of the nodes of the k8s clusterminikube statuskubectl version to find the version of kubernetesMinikube CLI … for start up/deleting the clusterKebectl CLI …for configuring the Minikube clusterCreate Deployment kubectl get all to get all the components running inside the clusterkubectl get nodeskubectl get podkubectl get serviceskubectl create deployment NAME —image=image [—dry-run] [options]e.g. kubectl create deployment nginx-depl —image=nginxkubectl get deploymentkubectl get podkubectl delete deployment to delete the deployment along with pods and replicaset related to the deploymentDebugging Podskubectl logs pod-namekubectl describe pod pod-namekubectl exec -it pod-name — bin/bash entr the container and look at the terminalKubectl apply -Finstead of create deployment with using command line, we can use the configuration fileskubectly apply -f config-file.yaml if deployment already created/ it updates with the new configskubectl delete -f config-file.yaml to delete the deploymentWhy YamlYaml Configuration file explainedEach configuratioin file has 3 parts Metadata Specs —&gt; attributes are specific to the type of components Status —&gt; automatically generated and added by kubernetesFirst two lines will be what component we want to createapiVersion fieldkind Versionpod v1service v1Replicaset apps/v1Deployment apps/v1Kind —- refers to the type of different objects, it may be pod, ReplicaSet, ReplicationController, Deploymentmetadata — the data about the object. useful when we have a lot of pods, and we can use these to substitue ports for those objectsspec —- Additional information of the objectKubectl service: kubectl apply -f nginx-service.yaml — to create Service using config filekubectl describe service nginx-service — to get thee information of port, target port and and pods ip which it will forward the requestskubectl get pod -o wide to check the ip addresses of the pod. shows more infokubectl get deployment nginx-deployment -o yaml &gt; nginx-deployment.yaml to get the current deployment configs in the yaml file, could be used to copy and do other deployments, but needs to be cleaned some of the unique attributesTo get the base 64 encoded strings like username and password to use in the secretecho -n 'someSecretString' | base64Order of creation of components in the Kubernetes matter. Create Secrets or configmaps if defined Create deploymentTypes of Service Internal Service or Cluster IP is Default Load Balance is for External communication service. needs to be defined in servic config file as typeTemplates MongoDB and internal service and secrets apiVersion: apps/v1 kind: Deployment metadata: name: mongodb-deployment labels: app: mongodb spec: replicas: 1 selector: matchLabels: app: mongodb template: metadata: labels: app: mongodb spec: containers: - name: mongodb image: mongo ports: - containerPort: 27017 env: - name: MONGO_INITDB_ROOT_USERNAME valueFrom: secretKeyRef: name: mongodb-secret key: mongo-root-username - name: MONGO_INITDB_ROOT_PASSWORD valueFrom: secretKeyRef: name: mongodb-secret key: mongo-root-password --- apiVersion: v1 kind: Service metadata: name: mongodb-service spec: selector: app: mongodb ports: - protocol: TCP port: 27017 targetPort: 27017 apiVersion: v1 kind: Secret metadata: name: mongodb-secret type: Opaque data: mongo-root-username: dXNlcm5hbWU= mongo-root-password: cGFzc3dvcmQ= Mongo express and External service and configmap apiVersion: apps/v1 kind: Deployment metadata: name: mongo-express labels: app: mongo-express spec: replicas: 1 selector: matchLabels: app: mongo-express template: metadata: labels: app: mongo-express spec: containers: - name: mongo-express image: mongo-express ports: - containerPort: 8081 env: - name: ME_CONFIG_MONGODB_ADMINUSERNAME valueFrom: secretKeyRef: name: mongodb-secret key: mongo-root-username - name: ME_CONFIG_MONGODB_ADMINPASSWORD valueFrom: secretKeyRef: name: mongodb-secret key: mongo-root-password - name: ME_CONFIG_MONGODB_SERVER valueFrom: configMapKeyRef: name: mongodb-configmap key: database_url --- apiVersion: v1 kind: Service metadata: name: mongo-express-service spec: selector: app: mongo-express type: LoadBalancer ports: - protocol: TCP port: 8081 targetPort: 8081 nodePort: 30000 apiVersion: v1 kind: ConfigMap metadata: name: mongodb-configmap data: database_url: mongodb-service After the external service is created, external ip will be shown in kubectl get services, in minikube need to run the kubectl service external-service-name to get the ip.NAMESPACE:What is a Namespace?organise resources in namespacevirtual cluster inside a kubernetes cluster4 names per Default kube-system : do not create or modify in kube-system, Sytem process, Master and kubectl processes Kube-public : public acccessible data; A configmap, which contains cluster informationWhy to use the namespace?Ingress:to use the ingress, need the ingress controller to evaluate the rules and forward the request.minikube addons enable ingressHelmpackage manager for kubernetes" } ]
